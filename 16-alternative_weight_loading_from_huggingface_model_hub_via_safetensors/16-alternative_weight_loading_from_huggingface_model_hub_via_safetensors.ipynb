{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992aced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.4.1\n",
      "torch version: 2.9.1\n",
      "safetensors version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "from safetensors.torch import load_file\n",
    "import torch,tiktoken,torch.nn as nn\n",
    "pkgs=['numpy','torch','safetensors']\n",
    "for p in pkgs:\n",
    "    print(f'{p} version: {version(p)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f80f5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves forward, but it's not enough.\n",
      "\n",
      "\"I'm not going to sit here and say, 'I'm not going to do this,'\n"
     ]
    }
   ],
   "source": [
    "BASE_CONFIG={'vocab_size':50257,\n",
    "             'context_length':1024,\n",
    "             'drop_rate':0,\n",
    "             'qkv_bias':True}\n",
    "model_configs={'gpt2-small (124M)':{'emb_dim':768,\n",
    "                                    'n_layers':12,\n",
    "                                    'n_heads':12}}\n",
    "CHOOSE_MODEL='gpt2-small (124M)'\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "URL_DIR={'gpt2-small (124M)':'gpt2'}\n",
    "output_file=f'model-{URL_DIR[CHOOSE_MODEL]}.safetensors'\n",
    "state_dict=load_file(output_file)\n",
    "def assign(left,right):\n",
    "    if left.shape!=right.shape:\n",
    "        raise ValueError(f'Shape mismatch.\\nLeft: {left.shape} | Right: {right.shape}')\n",
    "    return torch.nn.Parameter(right.detach())\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight=assign(gpt.pos_emb.weight,params['wpe.weight'])\n",
    "    gpt.tok_emb.weight=assign(gpt.tok_emb.weight,params[\"wte.weight\"])\n",
    "    for b in range(len(gpt.trf_blocks)):\n",
    "        q_w,k_w,v_w=torch.chunk(params[f'h.{b}.attn.c_attn.weight'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight=assign(gpt.trf_blocks[b].att.W_query.weight,\n",
    "                                                    q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight=assign(gpt.trf_blocks[b].att.W_key.weight,\n",
    "                                                  k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight=assign(gpt.trf_blocks[b].att.W_value.weight,\n",
    "                                                    v_w.T)\n",
    "        q_b,k_b,v_b=torch.chunk(params[f'h.{b}.attn.c_attn.bias'],3,axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias=assign(gpt.trf_blocks[b].att.W_query.bias,q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias=assign(gpt.trf_blocks[b].att.W_key.bias,k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias=assign(gpt.trf_blocks[b].att.W_value.bias,v_b)\n",
    "        gpt.trf_blocks[b].att.out_proj.weight=assign(gpt.trf_blocks[b].att.out_proj.weight,\n",
    "                                                     params[f'h.{b}.attn.c_proj.weight'].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias=assign(gpt.trf_blocks[b].att.out_proj.bias,params[f'h.{b}.attn.c_proj.bias'])\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight=assign(gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "                                                     params[f'h.{b}.mlp.c_fc.weight'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias=assign(gpt.trf_blocks[b].ff.layers[0].bias,params[f'h.{b}.mlp.c_fc.bias'])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight=assign(gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "                                                     params[f'h.{b}.mlp.c_proj.weight'].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias=assign(gpt.trf_blocks[b].ff.layers[2].bias,params[f'h.{b}.mlp.c_proj.bias'])\n",
    "        gpt.trf_blocks[b].norm1.scale=assign(gpt.trf_blocks[b].norm1.scale,params[f'h.{b}.ln_1.weight'])\n",
    "        gpt.trf_blocks[b].norm1.shift=assign(gpt.trf_blocks[b].norm1.shift,params[f'h.{b}.ln_1.bias'])\n",
    "        gpt.trf_blocks[b].norm2.scale=assign(gpt.trf_blocks[b].norm2.scale,params[f'h.{b}.ln_2.weight'])\n",
    "        gpt.trf_blocks[b].norm2.shift=assign(gpt.trf_blocks[b].norm2.shift,params[f'h.{b}.ln_2.bias'])\n",
    "    gpt.final_norm.scale=assign(gpt.final_norm.scale,params['ln_f.weight'])\n",
    "    gpt.final_norm.shift=assign(gpt.final_norm.shift,params['ln_f.bias'])\n",
    "    gpt.out_head.weight=assign(gpt.out_head.weight,params['wte.weight'])\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0,'d_out must be divisible by n_heads.'\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "        keys=self.W_key(x)\n",
    "        queries=self.W_query(x)\n",
    "        values=self.W_value(x)\n",
    "        keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "        attn_scores=queries@keys.transpose(2,3)\n",
    "        mask_bool=self.mask.bool()[:num_tokens,\n",
    "                                   :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        context_vec=(attn_weights@values).transpose(1,2)\n",
    "        context_vec=context_vec.reshape(b,num_tokens,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return .5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+.044715*torch.pow(x,3))))\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),GELU(),nn.Linear(4*cfg['emb_dim'],cfg['emb_dim']))\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(d_in=cfg['emb_dim'],\n",
    "                                    d_out=cfg['emb_dim'],\n",
    "                                    context_length=cfg['context_length'],\n",
    "                                    num_heads=cfg['n_heads'],\n",
    "                                    dropout=cfg['drop_rate'],\n",
    "                                    qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2=LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid=nn.Dropout(cfg['drop_rate'])\n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_resid(x)\n",
    "        x=x+shortcut\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_resid(x)\n",
    "        x=x+shortcut\n",
    "        return x\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,\n",
    "                    keepdim=True)\n",
    "        var=x.var(dim=-1,\n",
    "                  keepdim=True,\n",
    "                  unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg['vocab_size'],\n",
    "                                  cfg['emb_dim'])\n",
    "        self.pos_emb=nn.Embedding(cfg['context_length'],\n",
    "                                  cfg['emb_dim'])\n",
    "        self.drop_emb=nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm=LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "        return logits\n",
    "gpt=GPTModel(BASE_CONFIG)\n",
    "device=torch.device('mps')\n",
    "load_weights_into_gpt(gpt,state_dict)\n",
    "gpt.to(device)\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "def generate(model,idx,max_new_tokens,context_size,temperature=0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx_cond)\n",
    "        logits=logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits,_=torch.topk(logits,top_k)\n",
    "            min_val=top_logits[:,-1]\n",
    "            logits=torch.where(logits<min_val,torch.tensor(float('-inf')).to(logits.device),logits)\n",
    "        if temperature>0:\n",
    "            logits=logits/temperature\n",
    "            logits=logits-logits.max(dim=-1,\n",
    "                                     keepdim=True).values\n",
    "            probs=torch.softmax(logits,dim=-1)\n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "        else:\n",
    "            idx_next=torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        if idx_next==eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx,idx_next),dim=1)\n",
    "    return idx\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "token_ids=generate(model=gpt.to(device),\n",
    "                   idx=text_to_token_ids('Every effort moves',tokenizer).to(device),\n",
    "                   max_new_tokens=30,\n",
    "                   context_size=BASE_CONFIG['context_length'],\n",
    "                   top_k=1,\n",
    "                   temperature=1)\n",
    "print(f'Output text: {token_ids_to_text(token_ids,tokenizer)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
