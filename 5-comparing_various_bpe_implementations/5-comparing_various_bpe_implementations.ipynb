{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23e98e0",
   "metadata": {},
   "source": [
    "# Using BPE From Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "from collections import deque,Counter\n",
    "from transformers import GPT2Tokenizer,GPT2TokenizerFast\n",
    "from importlib.metadata import version\n",
    "import os,json,tiktoken,transformers,regex as re\n",
    "print(f\"Tiktoken version: {version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9123cb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 995, 13, 198, 3792, 428, 257, 1332, 30]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tik_tokenizer=tiktoken.get_encoding('gpt2')\n",
    "text='''Hello, world.\n",
    "Is this a test?'''\n",
    "integers=tik_tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69d6566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world.\\nIs this a test?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings=tik_tokenizer.decode(integers)\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cb1062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tik_tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee8a91",
   "metadata": {},
   "source": [
    "# Using BPE Via HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6bd507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.0.0rc1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619174f",
   "metadata": {},
   "source": [
    "# Quick Performance Benchmark\n",
    "## Original OpenAI GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaae2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.93 ms ± 242 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    bs=list(range(ord('!'),ord('~')+1))+list(range(ord('¡'),ord('¬')+1))+list(range(ord('®'),ord('ÿ')+1))\n",
    "    cs=bs[:]\n",
    "    n=0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n+=1\n",
    "    cs=[chr(n) for n in cs]\n",
    "    return dict(zip(bs,cs))\n",
    "def get_pairs(word):\n",
    "    pairs=set()\n",
    "    prev_char=word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char,char))\n",
    "        prev_char=char\n",
    "    return pairs\n",
    "class Encoder:\n",
    "    def __init__(self,encoder,bpe_merges,errors='replace'):\n",
    "        self.encoder=encoder\n",
    "        self.decoder={v:k for k,v in self.encoder.items()}\n",
    "        self.errors=errors\n",
    "        self.byte_encoder=bytes_to_unicode()\n",
    "        self.byte_decoder={v:k for k,v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks=dict(zip(bpe_merges,range(len(bpe_merges))))\n",
    "        self.cache={}\n",
    "        self.pat=re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "    def bpe(self,token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word=tuple(token)\n",
    "        pairs=get_pairs(word)\n",
    "        if not pairs:\n",
    "            return token\n",
    "        while True:\n",
    "            bigram=min(pairs,key=lambda pair:self.bpe_ranks.get(pair,float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first,second=bigram\n",
    "            new_word=[]\n",
    "            i=0\n",
    "            while i<len(word):\n",
    "                try:\n",
    "                    j=word.index(first,i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i=j\n",
    "                except ValueError:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                if word[i]==first and i<len(word)-1 and word[i+1]==second:\n",
    "                    new_word.append(first+second)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i+=1\n",
    "            new_word=tuple(new_word)\n",
    "            word=new_word\n",
    "            if len(word)==1:\n",
    "                break\n",
    "            else:\n",
    "                pairs=get_pairs(word)\n",
    "        word=' '.join(word)\n",
    "        self.cache[token]=word\n",
    "        return word\n",
    "    def encode(self,text):\n",
    "        bpe_tokens=[]\n",
    "        for token in re.findall(self.pat,text):\n",
    "            token=''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "    def decode(self,tokens):\n",
    "        text=''.join([self.decoder[token] for token in tokens])\n",
    "        text=bytearray([self.byte_decoder[c] for c in text]).decode('utf-8',errors=self.errors)\n",
    "        return text\n",
    "def get_encoder(model_name,models_dir):\n",
    "    with open(os.path.join(models_dir,model_name,'encoder.json'),'r') as f:\n",
    "        encoder=json.load(f)\n",
    "    with open(os.path.join(models_dir,model_name,'vocab.bpe'),'r',encoding='utf-8') as f:\n",
    "        bpe_data=f.read()\n",
    "    bpe_merges=[tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(encoder=encoder,\n",
    "                   bpe_merges=bpe_merges)\n",
    "orig_tokenizer=get_encoder(model_name='gpt2_model',\n",
    "                           models_dir='.')\n",
    "with open('../1-adding_bells_whistles_to_training_loop/verdict.txt','r',encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "%timeit orig_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b8fc8",
   "metadata": {},
   "source": [
    "## Tiktoken OpenAI GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6761999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43 ms ± 5.94 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tik_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba052d30",
   "metadata": {},
   "source": [
    "# HuggingFace OpenAI GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d9ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.91 ms ± 207 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "%timeit hf_tokenizer(raw_text)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7353619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.53 ms ± 42 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer(raw_text,max_length=5145,truncation=True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b68e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.52 ms ± 27.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer_fast=GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "%timeit hf_tokenizer_fast(raw_text)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46df323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.52 ms ± 31.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer_fast(raw_text,max_length=5145,truncation=True)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890473c",
   "metadata": {},
   "source": [
    "# GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ad47bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.3 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        self.inverse_vocab={}\n",
    "        self.bpe_merges={}\n",
    "        self.bpe_ranks={}\n",
    "    def train(self,text,vocab_size,allowed_special={'<|endoftext|>'}):\n",
    "        processed_text=[]\n",
    "        for i,char in enumerate(text):\n",
    "            if char==' ' and i!=0:\n",
    "                processed_text.append('Ġ')\n",
    "            if char!=' ':\n",
    "                processed_text.append(char)\n",
    "        processed_text=''.join(processed_text)\n",
    "        unique_chars=[chr(i) for i in range(256)]\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "        self.vocab={i:char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab={char:i for i,char in self.vocab.items()}\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id=len(self.vocab)\n",
    "                    self.vocab[new_id]=token\n",
    "                    self.inverse_vocab[token]=new_id\n",
    "        token_ids=[self.inverse_vocab[char] for char in processed_text]\n",
    "        for new_id in range(len(self.vocab),vocab_size):\n",
    "            pair_id=self.find_freq_pair(token_ids,mode='most')\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids=self.replace_pair(token_ids,pair_id,new_id)\n",
    "            self.bpe_merges[pair_id]=new_id\n",
    "        for (p0,p1),new_id in self.bpe_merges.items():\n",
    "            merged_token=self.vocab[p0]+self.vocab[p1]\n",
    "            self.vocab[new_id]=merged_token\n",
    "            self.inverse_vocab[merged_token]=new_id\n",
    "    def load_vocab_and_merges_from_openai(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'r',encoding='utf-8') as file:\n",
    "            loaded_vocab=json.load(file)\n",
    "            self.vocab={int(v):k for k,v in loaded_vocab.items()}\n",
    "            self.inverse_vocab={k:int(v) for k,v in loaded_vocab.items()}\n",
    "        if 'Ċ' not in self.inverse_vocab or self.inverse_vocab['Ċ']!=198:\n",
    "            raise KeyError(\"Vocabulary missing GPT-2 newline glyph 'Ċ' at id 198.\")\n",
    "        if '<|endoftext|>' not in self.inverse_vocab or self.inverse_vocab['<|endoftext|>']!=50256:\n",
    "            raise KeyError('Vocabulary missing <|endoftext|> at id 50256.')\n",
    "        if '\\n' not in self.inverse_vocab:\n",
    "            self.inverse_vocab['\\n']=self.inverse_vocab['Ċ']\n",
    "        if '\\r' not in self.inverse_vocab:\n",
    "            if 201 in self.vocab:\n",
    "                self.inverse_vocab['\\r']=201\n",
    "            else:\n",
    "                raise KeyError('Vocabulary missing carriage return token at id 201.')\n",
    "        self.bpe_ranks={}\n",
    "        with open(bpe_merges_path,'r',encoding='utf-8') as file:\n",
    "            lines=file.readlines()\n",
    "            if lines and lines[0].startswith('#'):\n",
    "                lines=lines[1:]\n",
    "            rank=0\n",
    "            for line in lines:\n",
    "                token1,*rest=line.strip().split()\n",
    "                if len(rest)!=1:\n",
    "                    continue\n",
    "                token2=rest[0]\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    self.bpe_ranks[(token1,token2)]=rank\n",
    "                    rank+=1\n",
    "                else:\n",
    "                    pass\n",
    "    def encode(self,text,allowed_special=None):\n",
    "        specials_in_vocab=[tok for tok in self.inverse_vocab if tok.startswith('<|') and tok.endswith('|>')]\n",
    "        if allowed_special is None:\n",
    "            disallowed=[tok for tok in specials_in_vocab if tok in text]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        else:\n",
    "            disallowed=[tok for tok in specials_in_vocab if tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        token_ids=[]\n",
    "        if allowed_special is not None and len(allowed_special)>0:\n",
    "            special_pattern='('+'|'.join(re.escape(tok) for tok in sorted(allowed_special,key=len,reverse=True))+')'\n",
    "            last_index=0\n",
    "            for match in re.finditer(special_pattern,text):\n",
    "                prefix=text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix,allowed_special=None))\n",
    "                special_token=match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f'Special token {special_token} not found in vocabulary.')\n",
    "                last_index=match.end()\n",
    "            text=text[last_index:]\n",
    "            disallowed=[tok for tok in self.inverse_vocab if tok.startswith('<|') and tok.endswith('|>') and tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        tokens=[]\n",
    "        parts=re.split(r'(\\r\\n|\\r|\\n)',text)\n",
    "        for part in parts:\n",
    "            if part=='':\n",
    "                continue\n",
    "            if part=='\\r\\n':\n",
    "                tokens.append('\\r')\n",
    "                tokens.append('\\n')\n",
    "                continue\n",
    "            if part=='\\r':\n",
    "                tokens.append('\\r')\n",
    "                continue\n",
    "            if part=='\\n':\n",
    "                tokens.append('\\n')\n",
    "                continue\n",
    "            pending_spaces=0\n",
    "            for m in re.finditer(r'( +)|(\\S+)',part):\n",
    "                if m.group(1) is not None:\n",
    "                    pending_spaces+=len(m.group(1))\n",
    "                else:\n",
    "                    word=m.group(2)\n",
    "                    if pending_spaces>0:\n",
    "                        tokens.append('Ġ'+word)\n",
    "                        for _ in range(pending_spaces-1):\n",
    "                            tokens.append('Ġ')\n",
    "                        pending_spaces=0\n",
    "                    else:\n",
    "                        tokens.append(word)\n",
    "            for _ in range(pending_spaces):\n",
    "                tokens.append('Ġ')\n",
    "        for tok in tokens:\n",
    "            if tok in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[tok])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(tok))\n",
    "        return token_ids\n",
    "    def tokenize_with_bpe(self,token):\n",
    "        token_ids=[self.inverse_vocab.get(char,None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars=[char for char,tid in zip(token,token_ids) if tid is None]\n",
    "            raise ValueError(f'Characters not found in vocab: {missing_chars}')\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge=True\n",
    "            while can_merge and len(token_ids)>1:\n",
    "                can_merge=False\n",
    "                new_tokens=[]\n",
    "                i=0\n",
    "                while i<len(token_ids)-1:\n",
    "                    pair=(token_ids[i],\n",
    "                          token_ids[i+1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id=self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        i+=2\n",
    "                        can_merge=True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i+=1\n",
    "                if i<len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids=new_tokens\n",
    "            return token_ids\n",
    "        symbols=[self.vocab[id_num] for id_num in token_ids]\n",
    "        while True:\n",
    "            pairs=set(zip(symbols,symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "            min_rank=float('inf')\n",
    "            bigram=None\n",
    "            for p in pairs:\n",
    "                r=self.bpe_ranks.get(p,float('inf'))\n",
    "                if r<min_rank:\n",
    "                    min_rank=r\n",
    "                    bigram=p\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first,second=bigram\n",
    "            new_symbols=[]\n",
    "            i=0\n",
    "            while i<len(symbols):\n",
    "                if i<len(symbols)-1 and symbols[i]==first and symbols[i+1]==second:\n",
    "                    new_symbols.append(first+second)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i+=1\n",
    "            symbols=new_symbols\n",
    "            if len(symbols)==1:\n",
    "                break\n",
    "        merged_ids=[self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "    def decode(self,token_ids):\n",
    "        out=[]\n",
    "        for tid in token_ids:\n",
    "            if tid not in self.vocab:\n",
    "                raise ValueError(f'Token ID {tid} not found in vocab.')\n",
    "            tok=self.vocab[tid]\n",
    "            if tid==198 or tok=='\\n':\n",
    "                out.append('\\n')\n",
    "            elif tid==201 or tok=='\\r':\n",
    "                out.append('\\r')\n",
    "            elif tok.startswith('Ġ'):\n",
    "                out.append(' '+tok[1:])\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return ''.join(out)\n",
    "    def save_vocab_and_merges(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'w',encoding='utf-8') as file:\n",
    "            json.dump(self.vocab,file,ensure_ascii=False,indent=2)\n",
    "        with open(bpe_merges_path,'w',encoding='utf-8') as file:\n",
    "            merges_list=[{'pair':list(pair),\n",
    "                          'new_id':new_id} for pair,new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list,file,ensure_ascii=False,indent=2)\n",
    "    def load_vocab_and_merges(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'r',encoding='utf-8') as file:\n",
    "            loaded_vocab=json.load(file)\n",
    "            self.vocab={int(k):v for k,v in loaded_vocab.items()}\n",
    "            self.inverse_vocab={v:int(k) for k,v in loaded_vocab.items()}\n",
    "        with open(bpe_merges_path,'r',encoding='utf-8') as file:\n",
    "            merges_list=json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair=tuple(merge['pair'])\n",
    "                new_id=merge['new_id']\n",
    "                self.bpe_merges[pair]=new_id\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self,token):\n",
    "        return self.inverse_vocab.get(token,None)\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids,mode='most'):\n",
    "        pairs=Counter(zip(token_ids,token_ids[1:]))\n",
    "        if not pairs:\n",
    "            return None\n",
    "        if mode=='most':\n",
    "            return max(pairs.items(),key=lambda x:x[1])[0]\n",
    "        elif mode=='least':\n",
    "            return min(pairs.items(),key=lambda x:x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode: Choose 'most', 'least'.\")\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids,pair_id,new_id):\n",
    "        dq=deque(token_ids)\n",
    "        replaced=[]\n",
    "        while dq:\n",
    "            current=dq.popleft()\n",
    "            if dq and (current,dq[0])==pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        return replaced\n",
    "tokenizer_gpt2=BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(vocab_path=os.path.join('gpt2_model','encoder.json'),\n",
    "                                                 bpe_merges_path=os.path.join('gpt2_model','vocab.bpe'))\n",
    "%timeit tokenizer_gpt2.encode(raw_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
