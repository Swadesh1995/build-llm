{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71412b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch,tiktoken,torch.nn as nn\n",
    "print(f\"Torch version: {version('torch')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a49f1",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2facfef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "        token_ids=tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx],self.target_ids[idx]\n",
    "def create_dataloader(txt,batch_size=4,max_length=256,stride=128,shuffle=True):\n",
    "    tokenizer=tiktoken.get_encoding('gpt2')\n",
    "    dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "    dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "    return dataloader\n",
    "with open('small-text-sample.txt','r',encoding='utf-8') as f:\n",
    "    raw_text=f.read()\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n",
    "encoded_text=tokenizer.encode(raw_text)\n",
    "vocab_size=50257\n",
    "output_dim=256\n",
    "max_len=1024\n",
    "context_length=max_len\n",
    "token_embedding_layer=nn.Embedding(vocab_size,output_dim)\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,output_dim)\n",
    "max_length=4\n",
    "dataloader=create_dataloader(raw_text,batch_size=8,max_length=max_length,stride=max_length)\n",
    "for batch in dataloader:\n",
    "    x,y=batch\n",
    "    token_embeddings=token_embedding_layer(x)\n",
    "    pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings=token_embeddings+pos_embeddings\n",
    "    break\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e688c3c",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "## Alternative Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078a60ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0,'d_out must be divisible by num_heads.'\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "        keys=self.W_key(x)\n",
    "        queries=self.W_query(x)\n",
    "        values=self.W_value(x)\n",
    "        keys=keys.view(b,num_tokens,self.num_heads,self.head_dim) \n",
    "        values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "        attn_scores=queries@keys.transpose(2,3)\n",
    "        mask_bool=self.mask.bool()[:num_tokens,\n",
    "                                   :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        context_vec=(attn_weights@values).transpose(1,2) \n",
    "        context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "context_length=max_length\n",
    "d_in=output_dim\n",
    "d_out=d_in\n",
    "mha=MultiHeadAttention(d_in,d_out,context_length,0,num_heads=2)\n",
    "batch=input_embeddings\n",
    "context_vecs=mha(batch)\n",
    "print(f'context_vecs.shape: {context_vecs.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
