{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9d02d0",
   "metadata": {},
   "source": [
    "# Main Idea Behind BPE\n",
    "## Bits, Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2cee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b\"This\\'s some text.\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "from collections import deque,Counter\n",
    "import os\n",
    "text=\"This's some text.\"\n",
    "byte_ary=bytearray(text,'utf-8')\n",
    "byte_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5df161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 39, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116, 46]\n",
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "ids=list(byte_ary)\n",
    "print(f'{ids}\\nNumber of characters: {len(text)}\\nNumber of token IDs: {len(ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d708dae",
   "metadata": {},
   "source": [
    "# BPE Implementation Walkthrough\n",
    "## Training, Encoding, Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e2623b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        self.inverse_vocab={}\n",
    "        self.bpe_merges={}\n",
    "    def train(self,text,vocab_size,allowed_special={'<|endoftext|>'}):\n",
    "        processed_text=[]\n",
    "        for i,char in enumerate(text):\n",
    "            if char==' ' and i!=0:\n",
    "                processed_text.append('Ġ')\n",
    "            if char!=' ':\n",
    "                processed_text.append(char)\n",
    "        processed_text=''.join(processed_text)\n",
    "        unique_chars=[chr(i) for i in range(256)]\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "        self.vocab={i:char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab={char:i for i,char in self.vocab.items()}\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id=len(self.vocab)\n",
    "                    self.vocab[new_id]=token\n",
    "                    self.inverse_vocab[token]=new_id\n",
    "        token_ids=[self.inverse_vocab[char] for char in processed_text]\n",
    "        for new_id in range(len(self.vocab),vocab_size):\n",
    "            pair_id=self.find_freq_pair(token_ids,mode='most')\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids=self.replace_pair(token_ids,pair_id,new_id)\n",
    "            self.bpe_merges[pair_id]=new_id\n",
    "        for (p0,p1),new_id in self.bpe_merges.items():\n",
    "            merged_token=self.vocab[p0]+self.vocab[p1]\n",
    "            self.vocab[new_id]=merged_token\n",
    "            self.inverse_vocab[merged_token]=new_id\n",
    "    def encode(self,text):\n",
    "        tokens=[]\n",
    "        words=text.replace('\\n',' \\n ').split()\n",
    "        for i,word in enumerate(words):\n",
    "            if i>0 and not word.startswith('\\n'):\n",
    "                tokens.append('Ġ'+word)\n",
    "            else:\n",
    "                tokens.append(word)\n",
    "        token_ids=[]\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_id=self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                sub_token_ids=self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "        return token_ids\n",
    "    def tokenize_with_bpe(self,token):\n",
    "        token_ids=[self.inverse_vocab.get(char,None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars=[char for char,tid in zip(token,token_ids) if tid is None]\n",
    "            raise ValueError(f'Characters not found in vocab: {missing_chars}')\n",
    "        can_merge=True\n",
    "        while can_merge and len(token_ids)>1:\n",
    "            can_merge=False\n",
    "            new_tokens=[]\n",
    "            i=0\n",
    "            while i<len(token_ids)-1:\n",
    "                pair=(token_ids[i],\n",
    "                      token_ids[i+1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id=self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    i+=2\n",
    "                    can_merge=True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i+=1\n",
    "            if i<len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids=new_tokens\n",
    "        return token_ids\n",
    "    def decode(self,token_ids):\n",
    "        decoded_string=''\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f'Token ID {token_id} not found in vocab.')\n",
    "            token=self.vocab[token_id]\n",
    "            if token.startswith('Ġ'):\n",
    "                decoded_string+=' '+token[1:]\n",
    "            else:\n",
    "                decoded_string+=token\n",
    "        return decoded_string\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self,token):\n",
    "        return self.inverse_vocab.get(token,None)\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids,mode='most'):\n",
    "        pairs=Counter(zip(token_ids,token_ids[1:]))\n",
    "        if mode=='most':\n",
    "            return max(pairs.items(),key=lambda x:x[1])[0]\n",
    "        elif mode=='least':\n",
    "            return min(pairs.items(),key=lambda x:x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode: Choose 'most', 'least'.\")\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids,pair_id,new_id):\n",
    "        dq=deque(token_ids)\n",
    "        replaced=[]\n",
    "        while dq:\n",
    "            current=dq.popleft()\n",
    "            if dq and (current,dq[0])==pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        return replaced\n",
    "with open('./1-adding_bells_whistles_to_training_loop/verdict.txt','r',encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "tokenizer=BPETokenizerSimple()\n",
    "tokenizer.train(text,vocab_size=1000,allowed_special={'<|endoftext|>'})\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284fce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.bpe_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b298b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 44, 256, 326, 972, 46]\n",
      "Number of characters: 39\n",
      "Number of token IDs: 19\n",
      "Jack embraced beauty through art, life.\n"
     ]
    }
   ],
   "source": [
    "input_text='Jack embraced beauty through art, life.'\n",
    "token_ids=tokenizer.encode(input_text)\n",
    "print(f'{token_ids}\\nNumber of characters: {len(input_text)}\\nNumber of token IDs: {len(token_ids)}\\n{tokenizer.decode(token_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c49d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "44 -> ,\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f'{token_id} -> {tokenizer.decode([token_id])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d560186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This's some text.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This's some text.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e7eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 44, 256, 326, 972, 46, 257]\n",
      "Number of characters: 52\n",
      "Number of token IDs: 20\n",
      "Jack embraced beauty through art, life.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import re,json\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        self.inverse_vocab={}\n",
    "        self.bpe_merges={}\n",
    "        self.bpe_ranks={}\n",
    "    def train(self,text,vocab_size,allowed_special={'<|endoftext|>'}):\n",
    "        processed_text=[]\n",
    "        for i,char in enumerate(text):\n",
    "            if char==' ' and i!=0:\n",
    "                processed_text.append('Ġ')\n",
    "            if char!=' ':\n",
    "                processed_text.append(char)\n",
    "        processed_text=''.join(processed_text)\n",
    "        unique_chars=[chr(i) for i in range(256)]\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "        self.vocab={i:char for i,char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab={char:i for i,char in self.vocab.items()}\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id=len(self.vocab)\n",
    "                    self.vocab[new_id]=token\n",
    "                    self.inverse_vocab[token]=new_id\n",
    "        token_ids=[self.inverse_vocab[char] for char in processed_text]\n",
    "        for new_id in range(len(self.vocab),vocab_size):\n",
    "            pair_id=self.find_freq_pair(token_ids,mode='most')\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids=self.replace_pair(token_ids,pair_id,new_id)\n",
    "            self.bpe_merges[pair_id]=new_id\n",
    "        for (p0,p1),new_id in self.bpe_merges.items():\n",
    "            merged_token=self.vocab[p0]+self.vocab[p1]\n",
    "            self.vocab[new_id]=merged_token\n",
    "            self.inverse_vocab[merged_token]=new_id\n",
    "    def load_vocab_and_merges_from_openai(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'r',encoding='utf-8') as file:\n",
    "            loaded_vocab=json.load(file)\n",
    "            self.vocab={int(v):k for k,v in loaded_vocab.items()}\n",
    "            self.inverse_vocab={k:int(v) for k,v in loaded_vocab.items()}\n",
    "        if 'Ċ' not in self.inverse_vocab or self.inverse_vocab['Ċ']!=198:\n",
    "            raise KeyError(\"Vocabulary missing GPT-2 newline glyph 'Ċ' at id 198.\")\n",
    "        if '<|endoftext|>' not in self.inverse_vocab or self.inverse_vocab['<|endoftext|>']!=50256:\n",
    "            raise KeyError('Vocabulary missing <|endoftext|> at id 50256.')\n",
    "        if '\\n' not in self.inverse_vocab:\n",
    "            self.inverse_vocab['\\n']=self.inverse_vocab['Ċ']\n",
    "        if '\\r' not in self.inverse_vocab:\n",
    "            if 201 in self.vocab:\n",
    "                self.inverse_vocab['\\r']=201\n",
    "            else:\n",
    "                raise KeyError('Vocabulary missing carriage return token at id 201.')\n",
    "        self.bpe_ranks={}\n",
    "        with open(bpe_merges_path,'r',encoding='utf-8') as file:\n",
    "            lines=file.readlines()\n",
    "            if lines and lines[0].startswith('#'):\n",
    "                lines=lines[1:]\n",
    "            rank=0\n",
    "            for line in lines:\n",
    "                token1,*rest=line.strip().split()\n",
    "                if len(rest)!=1:\n",
    "                    continue\n",
    "                token2=rest[0]\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    self.bpe_ranks[(token1,token2)]=rank\n",
    "                    rank+=1\n",
    "                else:\n",
    "                    pass\n",
    "    def encode(self,text,allowed_special=None):\n",
    "        specials_in_vocab=[tok for tok in self.inverse_vocab if tok.startswith('<|') and tok.endswith('|>')]\n",
    "        if allowed_special is None:\n",
    "            disallowed=[tok for tok in specials_in_vocab if tok in text]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        else:\n",
    "            disallowed=[tok for tok in specials_in_vocab if tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        token_ids=[]\n",
    "        if allowed_special is not None and len(allowed_special)>0:\n",
    "            special_pattern='('+'|'.join(re.escape(tok) for tok in sorted(allowed_special,key=len,reverse=True))+')'\n",
    "            last_index=0\n",
    "            for match in re.finditer(special_pattern,text):\n",
    "                prefix=text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix,allowed_special=None))\n",
    "                special_token=match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f'Special token {special_token} not found in vocabulary.')\n",
    "                last_index=match.end()\n",
    "            text=text[last_index:]\n",
    "            disallowed=[tok for tok in self.inverse_vocab if tok.startswith('<|') and tok.endswith('|>') and tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens encountered in text: {disallowed}')\n",
    "        tokens=[]\n",
    "        parts=re.split(r'(\\r\\n|\\r|\\n)',text)\n",
    "        for part in parts:\n",
    "            if part=='':\n",
    "                continue\n",
    "            if part=='\\r\\n':\n",
    "                tokens.append('\\r')\n",
    "                tokens.append('\\n')\n",
    "                continue\n",
    "            if part=='\\r':\n",
    "                tokens.append('\\r')\n",
    "                continue\n",
    "            if part=='\\n':\n",
    "                tokens.append('\\n')\n",
    "                continue\n",
    "            pending_spaces=0\n",
    "            for m in re.finditer(r'( +)|(\\S+)',part):\n",
    "                if m.group(1) is not None:\n",
    "                    pending_spaces+=len(m.group(1))\n",
    "                else:\n",
    "                    word=m.group(2)\n",
    "                    if pending_spaces>0:\n",
    "                        tokens.append('Ġ'+word)\n",
    "                        for _ in range(pending_spaces-1):\n",
    "                            tokens.append('Ġ')\n",
    "                        pending_spaces=0\n",
    "                    else:\n",
    "                        tokens.append(word)\n",
    "            for _ in range(pending_spaces):\n",
    "                tokens.append('Ġ')\n",
    "        for tok in tokens:\n",
    "            if tok in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[tok])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(tok))\n",
    "        return token_ids\n",
    "    def tokenize_with_bpe(self,token):\n",
    "        token_ids=[self.inverse_vocab.get(char,None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars=[char for char,tid in zip(token,token_ids) if tid is None]\n",
    "            raise ValueError(f'Characters not found in vocab: {missing_chars}')\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge=True\n",
    "            while can_merge and len(token_ids)>1:\n",
    "                can_merge=False\n",
    "                new_tokens=[]\n",
    "                i=0\n",
    "                while i<len(token_ids)-1:\n",
    "                    pair=(token_ids[i],\n",
    "                          token_ids[i+1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id=self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        i+=2\n",
    "                        can_merge=True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i+=1\n",
    "                if i<len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids=new_tokens\n",
    "            return token_ids\n",
    "        symbols=[self.vocab[id_num] for id_num in token_ids]\n",
    "        while True:\n",
    "            pairs=set(zip(symbols,symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "            min_rank=float('inf')\n",
    "            bigram=None\n",
    "            for p in pairs:\n",
    "                r=self.bpe_ranks.get(p,float('inf'))\n",
    "                if r<min_rank:\n",
    "                    min_rank=r\n",
    "                    bigram=p\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first,second=bigram\n",
    "            new_symbols=[]\n",
    "            i=0\n",
    "            while i<len(symbols):\n",
    "                if i<len(symbols)-1 and symbols[i]==first and symbols[i+1]==second:\n",
    "                    new_symbols.append(first+second)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i+=1\n",
    "            symbols=new_symbols\n",
    "            if len(symbols)==1:\n",
    "                break\n",
    "        merged_ids=[self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "    def decode(self,token_ids):\n",
    "        out=[]\n",
    "        for tid in token_ids:\n",
    "            if tid not in self.vocab:\n",
    "                raise ValueError(f'Token ID {tid} not found in vocab.')\n",
    "            tok=self.vocab[tid]\n",
    "            if tid==198 or tok=='\\n':\n",
    "                out.append('\\n')\n",
    "            elif tid==201 or tok=='\\r':\n",
    "                out.append('\\r')\n",
    "            elif tok.startswith('Ġ'):\n",
    "                out.append(' '+tok[1:])\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return ''.join(out)\n",
    "    def save_vocab_and_merges(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'w',encoding='utf-8') as file:\n",
    "            json.dump(self.vocab,file,ensure_ascii=False,indent=2)\n",
    "        with open(bpe_merges_path,'w',encoding='utf-8') as file:\n",
    "            merges_list=[{'pair':list(pair),\n",
    "                          'new_id':new_id} for pair,new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list,file,ensure_ascii=False,indent=2)\n",
    "    def load_vocab_and_merges(self,vocab_path,bpe_merges_path):\n",
    "        with open(vocab_path,'r',encoding='utf-8') as file:\n",
    "            loaded_vocab=json.load(file)\n",
    "            self.vocab={int(k):v for k,v in loaded_vocab.items()}\n",
    "            self.inverse_vocab={v:int(k) for k,v in loaded_vocab.items()}\n",
    "        with open(bpe_merges_path,'r',encoding='utf-8') as file:\n",
    "            merges_list=json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair=tuple(merge['pair'])\n",
    "                new_id=merge['new_id']\n",
    "                self.bpe_merges[pair]=new_id\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self,token):\n",
    "        return self.inverse_vocab.get(token,None)\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids,mode='most'):\n",
    "        pairs=Counter(zip(token_ids,token_ids[1:]))\n",
    "        if not pairs:\n",
    "            return None\n",
    "        if mode=='most':\n",
    "            return max(pairs.items(),key=lambda x:x[1])[0]\n",
    "        elif mode=='least':\n",
    "            return min(pairs.items(),key=lambda x:x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode: Choose 'most', 'least'.\")\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids,pair_id,new_id):\n",
    "        dq=deque(token_ids)\n",
    "        replaced=[]\n",
    "        while dq:\n",
    "            current=dq.popleft()\n",
    "            if dq and (current,dq[0])==pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        return replaced\n",
    "tokenizer=BPETokenizerSimple()\n",
    "tokenizer.train(text,vocab_size=1000,allowed_special={'<|endoftext|>'})\n",
    "input_text='Jack embraced beauty through art, life.<|endoftext|>'\n",
    "token_ids=tokenizer.encode(input_text,allowed_special={'<|endoftext|>'})\n",
    "print(f'{token_ids}\\nNumber of characters: {len(input_text)}\\nNumber of token IDs: {len(token_ids)}\\n{tokenizer.decode(token_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae73518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "44 -> ,\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n",
      "257 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f'{token_id} -> {tokenizer.decode([token_id])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d72254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This's some text with \\n newline characters.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This's some text with \\n newline characters.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b09dda",
   "metadata": {},
   "source": [
    "## Loading Original GPT-2 BPE Tokenizer From OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f339c6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt2=BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(vocab_path=os.path.join('./5-comparing_various_bpe_implementations/gpt2_model','encoder.json'),\n",
    "                                                 bpe_merges_path=os.path.join('./5-comparing_various_bpe_implementations/gpt2_model','vocab.bpe'))\n",
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6097f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 338, 617, 2420, 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text=\"This's some text.\"\n",
    "token_ids=tokenizer_gpt2.encode(input_text)\n",
    "token_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
