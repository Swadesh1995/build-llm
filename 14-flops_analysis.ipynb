{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dfbc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from importlib.metadata import version\n",
    "import torch,torch.nn as nn\n",
    "pkgs=['thop','torch']\n",
    "for p in pkgs:\n",
    "    print(f'{p} version: {version(p)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5aab39",
   "metadata": {},
   "source": [
    "# Simple Benchmark With Fixed Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1b368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M)  : 5.1e+11 FLOPS\n",
      "gpt-medium (355M) : 1.4e+12 FLOPS\n",
      "gpt-large (774M)  : 3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    : 6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "BASE_CONFIG={'vocab_size':50257,\n",
    "             'context_length':1024,\n",
    "             'drop_rate':0,\n",
    "             'qkv_bias':True}\n",
    "model_configs={'gpt-small (124M)':{'emb_dim':768,\n",
    "                                   'n_layers':12,\n",
    "                                   'n_heads':12},\n",
    "               'gpt-medium (355M)':{'emb_dim':1024,\n",
    "                                   'n_layers':24,\n",
    "                                   'n_heads':16},\n",
    "               'gpt-large (774M)':{'emb_dim':1280,\n",
    "                                   'n_layers':36,\n",
    "                                   'n_heads':20},\n",
    "               'gpt-xl (1558M)':{'emb_dim':1600,\n",
    "                                 'n_layers':48,\n",
    "                                 'n_heads':25}}\n",
    "device=torch.device('mps')\n",
    "batch_size=2\n",
    "input_tensor=torch.randint(0,50257,(batch_size,1024)).to(device)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,\n",
    "                    keepdim=True)\n",
    "        var=x.var(dim=-1,\n",
    "                  keepdim=True,\n",
    "                  unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return .5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+.044715*torch.pow(x,3))))\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),GELU(),nn.Linear(4*cfg['emb_dim'],cfg['emb_dim']))\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0,'d_out must be divisible by n_heads.'\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "        keys=self.W_key(x)\n",
    "        queries=self.W_query(x)\n",
    "        values=self.W_value(x)\n",
    "        keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "        attn_scores=queries@keys.transpose(2,3)\n",
    "        mask_bool=self.mask.bool()[:num_tokens,\n",
    "                                   :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        context_vec=(attn_weights@values).transpose(1,2)\n",
    "        context_vec=context_vec.reshape(b,num_tokens,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(d_in=cfg['emb_dim'],\n",
    "                                    d_out=cfg['emb_dim'],\n",
    "                                    context_length=cfg['context_length'],\n",
    "                                    num_heads=cfg['n_heads'],\n",
    "                                    dropout=cfg['drop_rate'],\n",
    "                                    qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2=LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid=nn.Dropout(cfg['drop_rate'])\n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_resid(x)\n",
    "        x=x+shortcut\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_resid(x)\n",
    "        x=x+shortcut\n",
    "        return x\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg['vocab_size'],\n",
    "                                  cfg['emb_dim'])\n",
    "        self.pos_emb=nn.Embedding(cfg['context_length'],\n",
    "                                  cfg['emb_dim'])\n",
    "        self.drop_emb=nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm=LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "        return logits\n",
    "for size in model_configs:\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "    model=GPTModel(BASE_CONFIG).bfloat16()\n",
    "    model.to(device)\n",
    "    macs,params=profile(model,inputs=(input_tensor,),verbose=False)\n",
    "    flops=2*macs\n",
    "    print(f'{size:18}: {flops:.1e} FLOPS')\n",
    "    del model\n",
    "    torch.mps.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
